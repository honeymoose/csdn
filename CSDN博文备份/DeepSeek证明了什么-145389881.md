<p>AI 这个词现在很火爆，但 AI 是新技术吗？不认为 AI 是一个新的技术，其实这些概念的提出都是在几十年前的事情了。</p> <br><p>受限于当时的计算机计算能力，AI 很多概念都来源于人工神经网络，《人工智能》这门课程早在几十年前就是大学的必修课程了。</p> <br><p>二十世纪40年代后期，心理学家唐纳德·赫布根据神经可塑性的机制创造了一种对学习的假说，现在称作赫布型学习。赫布型学习被认为是一种典型的非监督式学习规则，它后来的变种是长期增强作用的早期模型。</p> <br><p>从1948年开始，研究人员将这种计算模型的思想应用到B型图灵机上。</p> <br><p>但受限于当时的计算机处理能力，很多概念无法实现。</p> <br><h3>对搜索的厌倦</h3> <br><p>人类对知识的获取总是希望越快，越准确越好。</p> <br><p>搜索引擎的作用是把所有所有索引到的内容通过一定的优先级进行排序的方式显示出来，但现实的过程中并没有对具体需要的内容进行分析和编排，导致很多内容无效。</p> <br><p>使用 Google，对于一些一般性的内容，通常都能找到不少的答案，但那个答案是正确的，需要使用搜索的人自己去判断，甚至尝试。</p> <br><p>在计算机里面举个例子，<strong>校验电话号码的正则表达式是什么？</strong></p> <br><p>如果之间使用搜索，会出现一堆结果，但那个结果是正确的，需要自己去验证。</p> <br><h4>AI 的作用</h4> <br><p>AI 针对上面的问题，进行了了处理，通常能够返回一个相对准确的结果。</p> <br><p>针对这个相对准确的结果能够降低在搜索使用时候的无力感。</p> <br><p></p> <br><p></p> <br><p class="img-center"><a href="https://cdn.isharkfly.com/com-isharkfly-www/discourse-uploads/original/3X/7/c/7c3cfa78eab524cb2e4e7222379b23d9b7fce318.jpeg" rel="nofollow"><img alt="2025-01-28_09-21-32" height="500" src="https://i-blog.csdnimg.cn/img_convert/922ca5a89be01160a0a7dce792586715.jpeg" width="413" /></a></p> <br><p>我们甚至可以把 AI 定义为：带有 API 功能的更高准确率搜索引擎。</p> <br><p>这个更高准确率是需要通过 LLM （大型语言模型）来训练后生成。</p> <br><h3>怎么训练 LLM</h3> <br><p>训练 LLM 需要计算能力，用土话来说，AI 对模型的训练算法需要 GPU 来通过更大的计算能力，CPU 也不是不可以，只是 GPU 的效果更好。</p> <br><p>这个主要也是根据 CPU 和 GPU 的特性和指令集来决定的。</p> <br><p></p> <br><p></p> <br><p class="img-center"><a href="https://cdn.isharkfly.com/com-isharkfly-www/discourse-uploads/original/3X/c/f/cf5524d8f5d724b791f52a7190bbca7915f39141.png" rel="nofollow"><img alt="CPU-and-GPU" height="466" src="https://i-blog.csdnimg.cn/img_convert/f2b8ba04243a1351c03e29e571df78ca.png" width="690" /></a></p> <br><p></p> <br><p>从上面的图片可以看到 GPU 在训练 AI 模型上比 CPU 更有优势。</p> <br><p>在这个时候，简单粗暴的办法就是堆性能，你的模型可能不是那么先进，也可能是里面代码是有不少可以优化的地方，也可以采取一些方法来避免过度的使用硬件性能。</p> <br><p>但在这一切都以快为基础的情况下，堆机器是最快的解决办法，这也很印度。</p> <br><p>软件不行，硬件来凑。</p> <br><p>华尔街在这里面看到了商机，也逐步的推高了英伟达的市场估值，好像现在没有英伟达的 高端 GPU 芯片模型都跑不了一样的。</p> <br><p>当然，对马斯克和华尔街来说是乐见其成的，他们能够通过这些概念来强化市场。</p> <br><p>前一段时间，AWS 来我们公司推销 LLM，最后什么都谈得还可以，唯独是这训练使用的机器谈不明白，因为 AWS 希望推荐使用最高性能的 GPU 优化后的机器来处理 LLM。</p> <br><p></p> <br><p></p> <br><p class="img-center"><a href="https://cdn.isharkfly.com/com-isharkfly-www/discourse-uploads/original/3X/2/a/2a0f3a2d13e69aca6a17b82adc4bbaefb197f9b0.jpeg" rel="nofollow"><img alt="2025-01-28_09-26-25" height="500" src="https://i-blog.csdnimg.cn/img_convert/12dd24d8474f8119f4412b02ef84bb72.jpeg" width="616" /></a></p> <br><p></p> <br><p>就上面这个配置，一小时需要 3 美元，训练的适合还不能只用这样一台机器，还需要多个 VPS 叠加 GPU。</p> <br><p>根据公司内部的资料完成训练的话，每个小时都几百美元的支出，这谁受得了。感觉 AWS 的目的就是来卖他们的 EC2 的，至于模型优化啥的都不是回事。</p> <br><h3>Deepseek</h3> <br><p>Deepseek 的做法就很中国。</p> <br><p>我们擅长于把一个产品做到市场上都没有竞争对手，最大的对手是自己。</p> <br><p>硬件差点意思，我们改软件。</p> <br><p>我们证明，虽然我们达不到 GPT 完全等同的效率，但也大差不差，最主要的是我们便宜。我们能做到极致的便宜。</p> <br><p>我们不需要高级的 GPU，就算是低性能的 GPU 我们也可以玩的。</p> <br><p>Deepseek 可以说是 AI 市场的一根搅屎棍，本来华尔街那边都在等着数钱，结果有人站出来说这 AI 也不需要那么复杂的计算能力，就是个普通小机器也是可以玩的。</p> <br><p>就好像说，Oracle 告诉你他们的数据库只适合跑小型机上，PGSQL 站起来说，就是个 4G 的虚拟机也可以玩数据库的高级功能的，性能大差不差。</p> <br><p>SQLite 更加不服了，我更小。</p> <br><p>对数据库和 LLM 来说，真实的需求是不同的，没有人能够承担数据丢失的损失，但 LLM 不同。</p> <br><p>那个模型便宜我就用那个模型训练，就算训练坏了，没事，原始数据没丢，换个模型重新来，只要训练速度足够快，价格足够便宜。</p> <br><p>Deepseek 的作用不在于说 Deepseek 真正有多强大，在于 Deepseek 把 AI 的一堆概念给整明白了，后来发现这东西也没那么玄乎，可能最后你可以在自己家都可以训练自己的 AI 了。</p> <br><p><a class="has-card" href="https://www.isharkfly.com/t/deepseek/16855" rel="nofollow" title="DeepSeek 证明了什么 - AI - iSharkFly"><span class="link-card-box" contenteditable="false"><span class="link-title">DeepSeek 证明了什么 - AI - iSharkFly</span><span class="link-desc">AI 这个词现在很火爆，但 AI 是新技术吗？不认为 AI 是一个新的技术，其实这些概念的提出都是在几十年前的事情了。 受限于当时的计算机计算能力，AI 很多概念都来源于人工神经网络，《人工智能》这门课程早在几十年前就是大学的必修课程了。 二十世纪40年代后期，心理学家唐纳德·赫布根据神经可塑性的机制创造了一种对学习的假说，现在称作赫布型学习。赫布型学习被认为是一种典型的非监督式学习规则，它后来的变种是长期增强作用的早期模型。 从…</span><span class="link-link"><img class="link-link-icon" src="https://csdnimg.cn/release/blog_editor_html/release2.3.7/ckeditor/plugins/CsdnLink/icons/icon-default.png?t=O83A" />https://www.isharkfly.com/t/deepseek/16855</span></span></a></p> <br><p></p>